{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d6552d-a117-4667-a77f-320d20d716d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709361f3-ae4c-4ff7-98ed-6b04a96077cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._subclasses.fake_tensor import FakeTensorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f6a878-7d32-4475-9e88-1ffafff1732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeTensor(..., size=(4, 2, 2))\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, device=\"cpu\")\n",
    "y = torch.empty(4, 2, 2, device=\"cpu\")\n",
    "with FakeTensorMode() as mode:\n",
    "    xf = mode.from_tensor(x)\n",
    "    yf = mode.from_tensor(y)\n",
    "    print(xf+yf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe20516d-0b0d-4507-8319-bb6d284a0e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C._DispatchOperatorHandle at 0x7f473855a8b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C._dispatch_find_schema_or_throw(\"aten::sin\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f6c6df2-718c-4b9c-9af1-64bc539aef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in python dispather?????\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pytree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 103\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#     fx_g = make_fx(f_to_trace, tracing_mode='fake')(x_a, x_b, y)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#     self.assertExpectedInline(fx_g.code, \"\"\"\\\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#     return (mul, mul_1, add)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#     \"\"\")\u001b[39;00m\n\u001b[1;32m    102\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisablePythonDispatcher()\n\u001b[0;32m--> 103\u001b[0m \u001b[43mtest_make_fx_with_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 90\u001b[0m, in \u001b[0;36mtest_make_fx_with_subclass\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     out1_unwrapped_attrs, _ \u001b[38;5;241m=\u001b[39m out1\u001b[38;5;241m.\u001b[39m__tensor_flatten__()\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mgetattr\u001b[39m(out1, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m out1_unwrapped_attrs], out2)\n\u001b[0;32m---> 90\u001b[0m \u001b[43mf_to_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 87\u001b[0m, in \u001b[0;36mtest_make_fx_with_subclass.<locals>.f_to_trace\u001b[0;34m(x_a, x_b, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_to_trace\u001b[39m(x_a, x_b, y):\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m TwoTensor(x_a, x_b)\n\u001b[0;32m---> 87\u001b[0m     out1, out2 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     out1_unwrapped_attrs, _ \u001b[38;5;241m=\u001b[39m out1\u001b[38;5;241m.\u001b[39m__tensor_flatten__()\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mgetattr\u001b[39m(out1, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m out1_unwrapped_attrs], out2)\n",
      "Cell \u001b[0;32mIn[13], line 75\u001b[0m, in \u001b[0;36mtest_make_fx_with_subclass.<locals>.f\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x, y):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Returns (TwoTensor, Tensor)\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m, y \u001b[38;5;241m+\u001b[39m y\n",
      "File \u001b[0;32m~/miniconda3/envs/torch210/lib/python3.8/site-packages/torch/_tensor.py:1386\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1386\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "Cell \u001b[0;32mIn[13], line 52\u001b[0m, in \u001b[0;36mTwoTensor.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 52\u001b[0m args_a \u001b[38;5;241m=\u001b[39m \u001b[43mpytree\u001b[49m\u001b[38;5;241m.\u001b[39mtree_map_only(TwoTensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39ma, args)\n\u001b[1;32m     53\u001b[0m args_b \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(TwoTensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mb, args)\n\u001b[1;32m     55\u001b[0m kwargs_a \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(TwoTensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39ma, kwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytree' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.fx.experimental.proxy_tensor import make_fx\n",
    "# torch/testing/_internal/two_tensor.py\n",
    "# A simple tensor subclass that holds two tensors internally, and runs every op on both tensors.\n",
    "class TwoTensor(torch.Tensor):\n",
    "    @staticmethod\n",
    "    def __new__(cls, a, b):\n",
    "        assert (\n",
    "            a.device == b.device\n",
    "            and a.layout == b.layout\n",
    "            and a.requires_grad == b.requires_grad\n",
    "            and a.dtype == b.dtype\n",
    "        )\n",
    "        # I guess it would be more accurate to represent the shape as torch.cat(a, b).shape\n",
    "        shape = a.shape\n",
    "        kwargs = {}\n",
    "        kwargs[\"strides\"] = a.stride()\n",
    "        kwargs[\"storage_offset\"] = a.storage_offset()\n",
    "        kwargs[\"device\"] = a.device\n",
    "        kwargs[\"layout\"] = a.layout\n",
    "        kwargs[\"requires_grad\"] = a.requires_grad\n",
    "        kwargs[\"dtype\"] = a.dtype\n",
    "        out = torch.Tensor._make_wrapper_subclass(cls, shape, **kwargs)\n",
    "\n",
    "        assert a.shape == b.shape\n",
    "        assert a.stride() == b.stride()\n",
    "        assert a.storage_offset() == b.storage_offset()\n",
    "        return out\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def __repr__(self):\n",
    "        a_repr = repr(self.a)\n",
    "        b_repr = repr(self.b)\n",
    "        return f\"TwoTensor({a_repr}, {b_repr})\"\n",
    "\n",
    "    def __tensor_flatten__(self):\n",
    "        return [\"a\", \"b\"], None\n",
    "\n",
    "    @staticmethod\n",
    "    def __tensor_unflatten__(inner_tensors, meta, outer_size, outer_stride):\n",
    "        assert meta is None\n",
    "        a, b = inner_tensors[\"a\"], inner_tensors[\"b\"]\n",
    "        return TwoTensor(a, b)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args, kwargs):\n",
    "        print(\"in python dispather?????\")\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        args_a = pytree.tree_map_only(TwoTensor, lambda x: x.a, args)\n",
    "        args_b = pytree.tree_map_only(TwoTensor, lambda x: x.b, args)\n",
    "\n",
    "        kwargs_a = pytree.tree_map_only(TwoTensor, lambda x: x.a, kwargs)\n",
    "        kwargs_b = pytree.tree_map_only(TwoTensor, lambda x: x.b, kwargs)\n",
    "\n",
    "        out_a = func(*args_a, **kwargs_a)\n",
    "        out_b = func(*args_b, **kwargs_b)\n",
    "        assert type(out_a) == type(out_b)\n",
    "        out_a_flat, spec = pytree.tree_flatten(out_a)\n",
    "        out_b_flat = pytree.tree_leaves(out_b)\n",
    "        # for aten ops that return non-tensors, just assume that\n",
    "        # our two inner tensors return the same value\n",
    "        out_flat = [\n",
    "            TwoTensor(o_a, o_b) if isinstance(o_a, torch.Tensor) else o_a\n",
    "            for o_a, o_b in zip(out_a_flat, out_b_flat)\n",
    "        ]\n",
    "        out = pytree.tree_unflatten(out_flat, spec)\n",
    "        return return_and_correct_aliasing(func, args, kwargs, out)\n",
    "        \n",
    "def test_make_fx_with_subclass() -> None:\n",
    "    def f(x, y):\n",
    "        # Returns (TwoTensor, Tensor)\n",
    "        return x * y, y + y\n",
    "    x_a = torch.zeros(4)\n",
    "    x_b = torch.zeros(4)\n",
    "    y = torch.ones(4)\n",
    "\n",
    "    # make_fx() is not responsible for unwrapping tensor subclass inputs,\n",
    "    # so we do it manually here.\n",
    "    # Why? In general, make_fx(f)(*args) promises that the graph returned has the same calling\n",
    "    # convention as f(*args). Unwrapping tensor subclass inputs can potentially change\n",
    "    # the number of input args to the graph, breaking that assumption\n",
    "    def f_to_trace(x_a, x_b, y):\n",
    "        x = TwoTensor(x_a, x_b)\n",
    "        out1, out2 = f(x, y)\n",
    "        out1_unwrapped_attrs, _ = out1.__tensor_flatten__()\n",
    "        return (*[getattr(out1, attr) for attr in out1_unwrapped_attrs], out2)\n",
    "    f_to_trace(x_a, x_b, y)\n",
    "#     fx_g = make_fx(f_to_trace, tracing_mode='fake')(x_a, x_b, y)\n",
    "#     self.assertExpectedInline(fx_g.code, \"\"\"\\\n",
    "\n",
    "\n",
    "\n",
    "# def forward(self, x_a_1, x_b_1, y_1):\n",
    "#     mul = torch.ops.aten.mul.Tensor(x_a_1, y_1);  x_a_1 = None\n",
    "#     mul_1 = torch.ops.aten.mul.Tensor(x_b_1, y_1);  x_b_1 = None\n",
    "#     add = torch.ops.aten.add.Tensor(y_1, y_1);  y_1 = None\n",
    "#     return (mul, mul_1, add)\n",
    "#     \"\"\")\n",
    "torch._C._DisablePythonDispatcher()\n",
    "test_make_fx_with_subclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd719dc6-aed4-4dbe-8996-1644c571ba89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
