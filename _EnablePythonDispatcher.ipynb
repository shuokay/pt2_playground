{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61112b2-b71d-414f-99c8-3d33f48d3a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4220,  0.5315], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2, requires_grad=True)\n",
    "r = torch._C._EnablePythonDispatcher()\n",
    "torch.add(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e71b0fb-d645-4bbb-a940-91e92746e2b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tried to register an operator (__test__::foo(Tensor x) -> Tensor) with the same name and overload name multiple times. Each overload's schema should only be registered with a single call to def(). Duplicate registration: registered at /dev/null:0. Original registration: registered at /dev/null:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonDispatcher\n\u001b[0;32m----> 2\u001b[0m dispatcher \u001b[38;5;241m=\u001b[39m \u001b[43mPythonDispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mregister([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(dispatcher\u001b[38;5;241m.\u001b[39mdispatchTable())\n",
      "File \u001b[0;32m~/miniconda3/envs/torch210/lib/python3.8/site-packages/torch/_python_dispatcher.py:75\u001b[0m, in \u001b[0;36mPythonDispatcher.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m C\u001b[38;5;241m.\u001b[39m_dispatch_check_invariants(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39m_dispatch_library(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFRAGMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdef_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfoo(Tensor x) -> Tensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tried to register an operator (__test__::foo(Tensor x) -> Tensor) with the same name and overload name multiple times. Each overload's schema should only be registered with a single call to def(). Duplicate registration: registered at /dev/null:0. Original registration: registered at /dev/null:0"
     ]
    }
   ],
   "source": [
    "from torch._python_dispatcher import PythonDispatcher\n",
    "dispatcher = PythonDispatcher()\n",
    "dispatcher.register([\"CPU\", \"XLA\", \"CompositeImplicitAutograd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f395324f-7f0f-4e2c-8fcb-02144e205292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed Dispatch Table\n",
      "key             kernel\n",
      "---------------------------\n",
      "CPU             fn_CPU [kernel]\n",
      "XLA             fn_XLA [kernel]\n",
      "Lazy            fn_CompositeImplicitAutograd [math kernel]\n",
      "FPGA            fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradOther   fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradCPU     [backend fallback]\n",
      "AutogradXLA     [backend fallback]\n",
      "AutogradLazy    fn_CompositeImplicitAutograd [math kernel]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch._C._DisablePythonDispatcher()\n",
    "print(dispatcher.dispatchTable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f21efc7-52e0-4b64-9025-40daf2d4ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed Dispatch Table\n",
      "key             kernel\n",
      "---------------------------\n",
      "CPU             fn_CPU [kernel]\n",
      "XLA             fn_XLA [kernel]\n",
      "Lazy            fn_CompositeImplicitAutograd [math kernel]\n",
      "FPGA            fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradOther   fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradCPU     [backend fallback]\n",
      "AutogradXLA     [backend fallback]\n",
      "AutogradLazy    fn_CompositeImplicitAutograd [math kernel]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch._C._EnablePythonDispatcher()\n",
    "print(dispatcher.dispatchTable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef34db2-be47-49a7-a25a-e79f34ad3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPU', 'AutogradCPU', 'FPGA', 'AutogradOther', 'XLA', 'AutogradXLA', 'Lazy', 'AutogradLazy', 'CompositeExplicitAutograd', 'Autograd', 'CompositeImplicitAutograd']\n"
     ]
    }
   ],
   "source": [
    "print(dispatcher.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9279bcb6-c682-46a4-82b8-20c6e8679672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registered Kernels\n",
      "key             kernel\n",
      "---------------------------\n",
      "CPU             fn_CPU\n",
      "XLA             fn_XLA\n",
      "CompositeImplicitAutograd[alias] fn_CompositeImplicitAutograd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dispatcher.registrations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64199de6-3e4f-434f-b088-1b11bf972062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: __test__::foo\n",
      "schema: __test__::foo(Tensor x) -> Tensor\n",
      "debug: registered at /dev/null:0\n",
      "alias analysis kind: FROM_SCHEMA\n",
      "CPU: fn_CPU :: (Tensor _0) -> Tensor _0 [ boxed unboxed ]\n",
      "XLA: fn_XLA :: (Tensor _0) -> Tensor _0 [ boxed unboxed ]\n",
      "CompositeImplicitAutograd[alias]: fn_CompositeImplicitAutograd :: (Tensor _0) -> Tensor _0 [ boxed unboxed ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dispatcher.rawRegistrations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4ea229-c978-427a-8359-c1ffbb25f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undefined: fn_CompositeImplicitAutograd [math kernel]\n",
      "CPU: fn_CPU [kernel]\n",
      "CUDA: fn_CompositeImplicitAutograd [math kernel]\n",
      "HIP: fn_CompositeImplicitAutograd [math kernel]\n",
      "XLA: fn_XLA [kernel]\n",
      "MPS: fn_CompositeImplicitAutograd [math kernel]\n",
      "IPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "XPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "HPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "VE: fn_CompositeImplicitAutograd [math kernel]\n",
      "Lazy: fn_CompositeImplicitAutograd [math kernel]\n",
      "MTIA: fn_CompositeImplicitAutograd [math kernel]\n",
      "PrivateUse1: fn_CompositeImplicitAutograd [math kernel]\n",
      "PrivateUse2: fn_CompositeImplicitAutograd [math kernel]\n",
      "PrivateUse3: fn_CompositeImplicitAutograd [math kernel]\n",
      "Meta: fn_CompositeImplicitAutograd [math kernel]\n",
      "FPGA: fn_CompositeImplicitAutograd [math kernel]\n",
      "ORT: fn_CompositeImplicitAutograd [math kernel]\n",
      "Vulkan: fn_CompositeImplicitAutograd [math kernel]\n",
      "Metal: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedCPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedCUDA: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedHIP: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedXLA: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedMPS: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedIPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedXPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedHPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedVE: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedLazy: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedMTIA: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedPrivateUse1: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedPrivateUse2: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedPrivateUse3: fn_CompositeImplicitAutograd [math kernel]\n",
      "QuantizedMeta: fn_CompositeImplicitAutograd [math kernel]\n",
      "CustomRNGKeyId: fn_CompositeImplicitAutograd [math kernel]\n",
      "MkldnnCPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseCPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseCUDA: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseHIP: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseXLA: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseMPS: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseIPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseXPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseHPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseVE: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseLazy: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseMTIA: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparsePrivateUse1: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparsePrivateUse2: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparsePrivateUse3: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseMeta: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseCsrCPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "SparseCsrCUDA: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorCPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorCUDA: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorHIP: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorXLA: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorMPS: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorIPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorXPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorHPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorVE: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorLazy: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorMTIA: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorPrivateUse1: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorPrivateUse2: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorPrivateUse3: fn_CompositeImplicitAutograd [math kernel]\n",
      "NestedTensorMeta: fn_CompositeImplicitAutograd [math kernel]\n",
      "BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\n",
      "Functionalize: fn_CompositeImplicitAutograd [math kernel]\n",
      "Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\n",
      "ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\n",
      "AutogradOther: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\n",
      "AutogradCUDA: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradHIP: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\n",
      "AutogradMPS: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradIPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradXPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradHPU: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradVE: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradLazy: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradMTIA: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradPrivateUse1: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradPrivateUse2: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradPrivateUse3: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradMeta: fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradNestedTensor: fn_CompositeImplicitAutograd [math kernel]\n",
      "Tracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\n",
      "AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\n",
      "FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\n",
      "Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\n",
      "PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\n",
      "PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\n",
      "PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dispatcher.rawDispatchTable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d599ebf7-c53f-4b38-8a4f-32078d70d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5863, 0.9270, 0.9825])\n",
      "tensor([0.5863, 0.9270, 0.9825])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch._custom_op.impl import custom_op\n",
    "import numpy as np\n",
    "from torch._dispatch.python import enable_crossref_functionalize\n",
    "torch._C._DisablePythonDispatcher()\n",
    "@custom_op(\"my_library::numpy_sin\")\n",
    "def numpy_sin(x: Tensor) -> Tensor:\n",
    "    ...\n",
    "# numpy_sin is now an instance of class CustomOp\n",
    "# print(type(numpy_sin))\n",
    "# Step 2: Register an implementation for various PyTorch subsystems\n",
    "# Register an implementation for CPU tensors\n",
    "@numpy_sin.impl('cpu')\n",
    "def numpy_sin_impl_cpu(x):\n",
    "    return torch.from_numpy(np.sin(x.numpy()))\n",
    "# Register an implementation for CUDA tensors\n",
    "@numpy_sin.impl('cuda')\n",
    "def numpy_sin_impl_cuda(x):\n",
    "    return torch.from_numpy(np.sin(x.cpu().numpy())).to(x.device)\n",
    "x = torch.randn(3)\n",
    "with enable_crossref_functionalize():\n",
    "    y = numpy_sin(x)  # calls numpy_sin_impl_cpu\n",
    "    print(y)\n",
    "    x_cuda = x.cuda()\n",
    "    \n",
    "    y = numpy_sin(x)  # calls numpy_sin_impl_cuda\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56fcb6b2-d534-4f9a-8a61-200acfd250a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self, a_1):\n",
      "    ones = torch.ops.aten.ones.default([4, 4], device = device(type='cpu'), pin_memory = False)\n",
      "    numpy_sin = torch.ops.my_library.numpy_sin.default(ones);  ones = None\n",
      "    matmul = torch.ops.aten.matmul.default(a_1, numpy_sin);  a_1 = numpy_sin = None\n",
      "    return matmul\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx.experimental.proxy_tensor import make_fx\n",
    "def test_pre_dispatch_mode_stack():\n",
    "    def f(a):\n",
    "        b = torch.ones(4, 4)\n",
    "        b = numpy_sin(b)\n",
    "        return torch.matmul(a, b)\n",
    "    # We expect to see matmul in the trace - it should NOT be decomposed into mm.\n",
    "    # Also, torch.ones() doesn't show up in the trace.\n",
    "    # This is annoying but expected: ones() never dispatches to the Autograd dispatch key,\n",
    "    # so our mode never sees it - it goes directly to the BackendSelect key.\n",
    "    inp = torch.ones(4, 4)\n",
    "    # Test that make_fx(pre_dispatch=True) clears caches properly.\n",
    "    from torch._dispatch.python import enable_python_dispatcher, no_python_dispatcher\n",
    "    with enable_python_dispatcher():\n",
    "        out1 = f(inp)\n",
    "    fx_g = make_fx(f, pre_dispatch=True)(inp)\n",
    "\n",
    "    print(fx_g.code.strip())\n",
    "\n",
    "test_pre_dispatch_mode_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73348ad6-1759-4e95-b2b1-103789864ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tried to register an operator (__test__::foo(Tensor x) -> Tensor) with the same name and overload name multiple times. Each overload's schema should only be registered with a single call to def(). Duplicate registration: registered at /dev/null:0. Original registration: registered at /dev/null:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonDispatcher\n\u001b[0;32m----> 2\u001b[0m dispatcher \u001b[38;5;241m=\u001b[39m \u001b[43mPythonDispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mregister([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch210/lib/python3.8/site-packages/torch/_python_dispatcher.py:75\u001b[0m, in \u001b[0;36mPythonDispatcher.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m C\u001b[38;5;241m.\u001b[39m_dispatch_check_invariants(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39m_dispatch_library(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFRAGMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdef_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfoo(Tensor x) -> Tensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tried to register an operator (__test__::foo(Tensor x) -> Tensor) with the same name and overload name multiple times. Each overload's schema should only be registered with a single call to def(). Duplicate registration: registered at /dev/null:0. Original registration: registered at /dev/null:0"
     ]
    }
   ],
   "source": [
    "from torch._python_dispatcher import PythonDispatcher\n",
    "dispatcher = PythonDispatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "639aa0a2-8108-416b-bf64-c9672eaf3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed Dispatch Table\n",
      "key             kernel\n",
      "---------------------------\n",
      "CPU             fn_CPU [kernel]\n",
      "XLA             fn_XLA [kernel]\n",
      "Lazy            fn_CompositeImplicitAutograd [math kernel]\n",
      "FPGA            fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradOther   fn_CompositeImplicitAutograd [math kernel]\n",
      "AutogradCPU     [backend fallback]\n",
      "AutogradXLA     [backend fallback]\n",
      "AutogradLazy    fn_CompositeImplicitAutograd [math kernel]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispatcher.register([\"CPU\", \"XLA\", \"CompositeImplicitAutograd\"])\n",
    "print(dispatcher.dispatchTable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378d24cb-fce8-455f-8369-1d07d0e0b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils._python_dispatch import _get_current_dispatch_mode_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99fac82-d370-49ab-b241-bfada15d6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_current_dispatch_mode_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77c4d3-4a6e-4c50-bdc5-c393462db025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
